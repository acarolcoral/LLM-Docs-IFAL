import os
import pdfplumber
from elasticsearch import Elasticsearch
from sentence_transformers import SentenceTransformer

# =====================================================
# ‚öôÔ∏è CONFIGURA√á√ïES INICIAIS
# =====================================================

# Nome do √≠ndice no Elasticsearch
INDEX = "documentos_ifal_llm"

# Conex√£o com o Elasticsearch local (sem autentica√ß√£o e sem HTTPS)
es = Elasticsearch("http://localhost:9200")

# Teste r√°pido de conex√£o
try:
    info = es.info()
    print(f"üîó Conectado ao Elasticsearch vers√£o {info['version']['number']}")
except Exception as e:
    print("‚ùå Erro ao conectar ao Elasticsearch:", e)
    exit(1)

# Modelo para gerar embeddings sem√¢nticos dos textos
model = SentenceTransformer("all-MiniLM-L6-v2")

# Pasta onde est√£o os PDFs a serem indexados
PASTA_PDFS = "documentos/pdfs"  

# =====================================================
# üßπ REINICIALIZA√á√ÉO DO √çNDICE (APAGA E RECRIA)
# =====================================================

if es.indices.exists(index=INDEX):
    print(f"üßπ Apagando √≠ndice existente: {INDEX}")
    es.indices.delete(index=INDEX)

# Cria o √≠ndice com mapeamento para texto e embeddings
es.indices.create(
    index=INDEX,
    body={
        "mappings": {
            "properties": {
                "arquivo": {"type": "keyword"},
                "conteudo": {"type": "text"},
                "embedding": {
                    "type": "dense_vector",
                    "dims": 384,  # tamanho do embedding gerado pelo modelo
                    "index": True,
                    "similarity": "cosine"
                }
            }
        }
    }
)

print(f"‚úÖ √çndice '{INDEX}' criado com sucesso.\n")

# =====================================================
# üìÑ LEITURA E INDEXA√á√ÉO DOS PDFs (AGORA INTEIROS)
# =====================================================

pdfs_processados = 0

for arquivo in os.listdir(PASTA_PDFS):
    if arquivo.endswith(".pdf"):
        caminho = os.path.join(PASTA_PDFS, arquivo)
        print(f"üìò Lendo arquivo: {arquivo}")

        texto_total = ""

        # Extrai texto de cada p√°gina do PDF
        try:
            with pdfplumber.open(caminho) as pdf:
                for pagina_num, pagina in enumerate(pdf.pages, 1):
                    texto_pagina = pagina.extract_text() or ""
                    texto_total += texto_pagina + "\n"
                    print(f"   üìÑ P√°gina {pagina_num}: {len(texto_pagina)} caracteres")
        except Exception as e:
            print(f"‚ùå Erro ao ler {arquivo}: {e}")
            continue

        if not texto_total.strip():
            print(f"‚ö†Ô∏è Nenhum texto encontrado em {arquivo}. Pulando arquivo.\n")
            continue

        # Remove espa√ßos extras e quebras de linha m√∫ltiplas
        texto_total = " ".join(texto_total.split())
        
        print(f"   üìä Texto extra√≠do: {len(texto_total)} caracteres")

        # Gera embedding do documento INTEIRO
        embedding = model.encode(texto_total).tolist()

        # Indexa o documento COMPLETO no Elasticsearch
        es.index(
            index=INDEX,
            document={
                "arquivo": arquivo,
                "conteudo": texto_total,
                "embedding": embedding
            }
        )

        pdfs_processados += 1
        print(f"‚úÖ {arquivo} indexado como documento completo.\n")

print(f"üèÅ Processamento conclu√≠do! {pdfs_processados} PDFs indexados como documentos completos.")